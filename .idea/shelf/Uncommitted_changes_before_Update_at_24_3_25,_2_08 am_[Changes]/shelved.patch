Index: Scripts/Subgroup_A/promo_events_analysis.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>## ----- Set up ----- ##\r\nimport numpy as np\r\nimport pandas as pd\r\nimport os\r\nfrom datetime import datetime, timedelta\r\nfrom emoji import demojize\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nimport nltk\r\nimport corpora\r\n\r\nfrom gensim.models import LdaModel\r\nfrom matplotlib.pyplot import thetagrids\r\nfrom scipy.stats import stats\r\n\r\n# ----- Required downloads -----\r\n# nltk.download('stopwords')\r\n# nltk.download('punkt_tab')\r\n\r\n## ----- Import review and promotion events datasets ----- ##\r\npath = os.getcwd()\r\n# df_reviews = pd.read_csv(path + '/data/universal_studio_branches.csv')\r\ndf_events = pd.read_csv(path + '/data/uss_promo_events.csv')\r\n\r\n## ----- Data cleaning ----- ##\r\n# (1) Check for missing values\r\ndef handle_missing_values(df, drop=True, fill_value=None):\r\n    missing_counts = df.isnull().sum()\r\n    if missing_counts.sum() == 0:\r\n        print(\"No missing values found.\")\r\n        return df\r\n    print(\"Missing values per column:\\n\", missing_counts)\r\n    if drop:\r\n        df = df.dropna()\r\n        print(\"Dropped rows with missing values.\")\r\n    elif fill_value is not None:\r\n        df = df.fillna(fill_value)\r\n        print(f\"Filled missing values with {fill_value}.\")\r\n\r\n    return df\r\n\r\ndf_reviews = handle_missing_values(df_reviews)\r\ndf_events = handle_missing_values(df_events)\r\n\r\n# (2) Convert dates to datetime type\r\ndf_reviews[\"written_date\"] = pd.to_datetime(df_reviews[\"written_date\"], errors='coerce')\r\ndf_events[\"start\"] = pd.to_datetime(df_events[\"start\"], format='%b %d, %Y', errors='coerce')\r\ndf_events[\"end\"] = pd.to_datetime(df_events[\"end\"], format='%b %d, %Y', errors='coerce')\r\n\r\ndf_events = df_events.sort_values(\"start\")  # order events by start date\r\n\r\n# (3) Handle duplicates\r\ndef remove_duplicates(df):\r\n    # check if there are duplicated rows\r\n    dup_count = df.duplicated().sum()\r\n    print(f\"Number of duplicate rows: {dup_count}\")\r\n\r\n    # drop duplicates if any\r\n    if dup_count > 0:\r\n        df.drop_duplicates(inplace=True)\r\n        print(\"Duplicates removed.\")\r\n    else:\r\n        print(\"No duplicates found.\")\r\n    return df\r\n\r\ndf_reviews = remove_duplicates(df_reviews)\r\ndf_events = remove_duplicates(df_events)\r\n\r\n## --- Data exploration --- ##    # edit: added data exploration section\r\n\"\"\"\r\nData exploration\r\n-------------------\r\nSince 'df_events' contains information on past promotion events of Universal Studios Singapore (USS),\r\nwe will filter the 'df_reviews' data to focus our analysis on USS reviews.\r\n\"\"\"\r\n# (1) Compute event duration\r\ndf_events[\"duration\"] = (df_events[\"end\"] - df_events[\"start\"]).dt.days\r\n\r\n# (2) Filter for reviews written during event period\r\ndef filter_reviews_during_events(reviews, events, branch = 'Universal Studios Singapore'):\r\n    event_reviews_dict = {}\r\n\r\n    for _, event in events.iterrows():\r\n        event_key = f\"{event['event']} ({event['start'].date()})\"  # event as unique key\r\n        start = event[\"start\"]\r\n        end = event[\"end\"]\r\n\r\n        filtered_reviews = reviews[\r\n            (reviews[\"written_date\"] >= start) &\r\n            (reviews[\"written_date\"] <= end) &\r\n            (reviews[\"branch\"] == branch)\r\n        ]\r\n\r\n        event_reviews_dict[event_key] = filtered_reviews\r\n\r\n    return event_reviews_dict\r\n\r\n# (3) Filter for reviews written before event period\r\ndef filter_reviews_before_events(reviews, events, branch = 'Universal Studios Singapore'):\r\n    event_reviews_dict = {}\r\n    for _, event in events.iterrows():\r\n        event_key = f\"{event['event']} ({event['start'].date()})\"  # event as unique key\r\n\r\n        # set the period before event to have the same duration for fair comparison\r\n        start = event[\"start\"] - timedelta(days=event[\"duration\"])\r\n        end = event[\"start\"] - timedelta(days=1)\r\n\r\n        filtered_reviews = reviews[\r\n            (reviews[\"written_date\"] >= start) &\r\n            (reviews[\"written_date\"] <= end) &\r\n            (reviews[\"branch\"] == branch)\r\n        ]\r\n\r\n        event_reviews_dict[event_key] = filtered_reviews\r\n\r\n    return event_reviews_dict\r\n\r\n# Call both functions to create 2 filtered dataframes\r\n# Structure of filtered df: {event_key: [df of filtered reviews]}\r\nreviews_during_events = filter_reviews_during_events(df_reviews, df_events)\r\nreviews_before_events = filter_reviews_before_events(df_reviews, df_events)\r\n\r\n# Drop unnecessary column\r\ndf_events = df_events.drop(columns=[\"duration\"])\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Scripts/Subgroup_A/promo_events_analysis.py b/Scripts/Subgroup_A/promo_events_analysis.py
--- a/Scripts/Subgroup_A/promo_events_analysis.py	(revision c4948aebfcfa93ca5d484d37b2c8703d266a2995)
+++ b/Scripts/Subgroup_A/promo_events_analysis.py	(date 1742750441287)
@@ -6,12 +6,7 @@
 from emoji import demojize
 import seaborn as sns
 import matplotlib.pyplot as plt
-import nltk
-import corpora
-
-from gensim.models import LdaModel
-from matplotlib.pyplot import thetagrids
-from scipy.stats import stats
+from scipy.stats import stats, shapiro
 
 # ----- Required downloads -----
 # nltk.download('stopwords')
@@ -19,7 +14,7 @@
 
 ## ----- Import review and promotion events datasets ----- ##
 path = os.getcwd()
-# df_reviews = pd.read_csv(path + '/data/universal_studio_branches.csv')
+df_reviews = pd.read_csv(path + '/data/universal_studio_branches.csv')
 df_events = pd.read_csv(path + '/data/uss_promo_events.csv')
 
 ## ----- Data cleaning ----- ##
@@ -66,9 +61,34 @@
 df_reviews = remove_duplicates(df_reviews)
 df_events = remove_duplicates(df_events)
 
-## --- Data exploration --- ##    # edit: added data exploration section
+## --- Sentiment Analysis --- ##
+"""
+Sentiment analysis: df_reviews 
+-------------------
+Using TextBlob to compute the polarity scores 
+We will be analysing the 'combined_text' = review title + text
 """
-Data exploration
+
+# Combine the text columns
+df_reviews['combined_text'] = df_reviews['title'] + " " + df_reviews['review_text']
+df_reviews = df_reviews.drop(columns=['title', 'review_text'])
+
+
+# (1) Text preprocessing: convert emojis
+df_reviews["combined_text"] = df_reviews["combined_text"].apply(lambda text: demojize(text))
+
+# (2) TextBlob: compute polarity scores
+from textblob import TextBlob
+
+def analyse_sentiment(df, text_column):
+    df["polarity"] = df[text_column].apply(lambda text: TextBlob(text).sentiment.polarity)
+    return df
+
+df_reviews = analyse_sentiment(df_reviews, 'combined_text')
+
+## --- Data Exploration --- ##
+"""
+Data Exploration: Filtering events
 -------------------
 Since 'df_events' contains information on past promotion events of Universal Studios Singapore (USS),
 we will filter the 'df_reviews' data to focus our analysis on USS reviews.
@@ -122,3 +142,169 @@
 
 # Drop unnecessary column
 df_events = df_events.drop(columns=["duration"])
+
+
+##
+"""
+Data Analysis: Change in polarity scores, review rating, review volume; Statistical analysis
+-------------------
+
+"""
+
+# (1) Compute difference in key metrics before event vs during event
+def compute_change_in_reviews(reviews_before_event, reviews_during_event):
+    changes = {}
+    for event, reviews_before in reviews_before_event.items():
+        # Get reviews during the event from the second dictionary
+        reviews_during = reviews_during_event.get(event)
+        if reviews_during is not None:
+            # Ensure the reviews are DataFrames before performing calculations
+            reviews_before = pd.DataFrame(reviews_before)
+            reviews_during = pd.DataFrame(reviews_during)
+
+            # Calculate average polarity score, review rating, and review volume before and during the event
+            before_avg = reviews_before[['polarity', 'rating']].mean()
+            during_avg = reviews_during[['polarity', 'rating']].mean()
+            before_volume = len(reviews_before)
+            during_volume = len(reviews_during)
+
+            # Calculate change in polarity score, review rating, and review volume
+            change = {
+                'review_polarity_change': during_avg['polarity'] - before_avg['polarity'],
+                'review_rating_change': during_avg['rating'] - before_avg['rating'],
+                'review_volume_change': during_volume - before_volume
+            }
+            changes[event] = change
+
+    return pd.DataFrame.from_dict(changes, orient='index')
+
+
+df_change_data = compute_change_in_reviews(reviews_before_events, reviews_during_events)
+
+# (2) Statistical analysis - understand whether event periods bring about statistically significant changes
+
+# Metrics to analyse
+metrics = {
+    "Review Rating Change": "review_rating_change",
+    "Review Polarity Change": "review_polarity_change",
+    "Review Volume Change": "review_volume_change"
+}
+
+for metric_name, col in metrics.items():
+    changes = df_change_data[col]
+
+    # Check normality using Shapiro-Wilk test
+    shapiro_stat, shapiro_p = stats.shapiro(changes)
+    print(f"Shapiro-Wilk Test for {metric_name}: p-value = {shapiro_p:.4f}")
+
+    # Choose appropriate test based on normality
+    if shapiro_p >= 0.05:
+        # Normally distributed: use one-sample t-test
+        test_stat, p_value = stats.ttest_1samp(changes, 0)
+        test_type = "T-test"
+    else:
+        # Not normally distributed: Use Wilcoxon signed-rank test
+        test_stat, p_value = stats.wilcoxon(changes)
+        test_type = "Wilcoxon Test"
+
+    # Print test results
+    print(f"{metric_name} ({test_type}): Test-statistic = {test_stat:.4f}, P-value = {p_value:.4f}")
+
+    # Print interpretation
+    if p_value < 0.05:
+        print(f"Significant change detected in {metric_name} before vs. during the event.\n")
+    else:
+        print(f"No significant change in {metric_name} before vs. during the event.\n")
+
+
+"""
+Analysis:
+-------------------
+There is no significant change in review volume, rating and polarity before vs during promotional events.
+
+"""
+
+
+##
+"""
+Data Visualisation: Plot the change data; Correlation matrix
+-------------------
+"""
+# ------ Visualise correlation matrix using heatmap ------
+correlation_matrix = df_change_data[['review_polarity_change', 'review_rating_change', 'review_volume_change']].corr()
+
+plt.figure(figsize=(8, 6))
+sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, linewidths=1, fmt='.2f', cbar=True)
+plt.title('Correlation Matrix of Review Polarity Change, Rating Change, and Volume Change')
+plt.tight_layout()
+plt.show()
+
+# ------ Visualise change using diverging bar plot ------
+
+def plot_review_change(ax, change_data, diff_col):
+    event_names = change_data.index.values
+    avg_change = change_data[diff_col]
+
+    df = pd.DataFrame({'event': event_names, 'difference': avg_change})
+    df['color'] = df['difference'].apply(lambda x: 'red' if x < 0 else 'blue')
+
+    ax.hlines(y=df.index, xmin=0, xmax=df['difference'], color=df['color'], alpha=0.7, linewidth=10)
+    ax.set_yticks(df.index)
+    ax.set_yticklabels(df['event'])
+    ax.set_xlabel(diff_col)
+    ax.set_title(f"{diff_col} before vs. during event periods")
+
+# fig 1. Individual plot for review_volume_change
+fig, ax = plt.subplots(figsize=(12, 8))
+plot_review_change(ax, df_change_data, "review_volume_change")
+plt.tight_layout()
+plt.show()
+
+# fig 2. Side-by-Side plot comparing review_polarity_change and review_rating_change
+fig, axes = plt.subplots(1, 2, figsize=(16, 8), sharey=True)
+plot_review_change(axes[0], df_change_data, "review_rating_change")
+plot_review_change(axes[1], df_change_data, "review_polarity_change")
+plt.tight_layout()
+plt.show()
+
+"""
+Analysis: 
+-------------
+Insights from correlation matrix:
+1. Review Volume and Sentiment, Review Volume and Ratings:
+    - Negligible negative correlation, which suggest that more reviews does not reflect guest satisfaction.
+    - While an increase in review volume typically signals higher engagement and interest in the campaign, it is not 
+    sufficient on its own to gauge guest satisfaction.
+    - To assess a campaign’s true impact, other factors, such as review sentiment and ratings, should be considered.
+    
+2.  Ratings and Sentiment
+    - Positive correlation, which indicates that higher ratings reflect better guest experience.
+    - To strengthen brand perception during event periods, campaigns should aim to create memorable experiences that not 
+    only generate reviews but also result in high ratings.
+ 
+
+Insights from visualisations:
+1. Plot of review_volume_change before vs during events:
+    - Comparing to the plots on review_rating_change and review_polarity_change, there is no visible trend. This 
+    aligns with our insights from the correlation matrix.
+
+2. Side-by-Side Plot of review_rating_change and review_polarity_change:
+    - Based on the plot, we can see that review_rating_change and review_polarity_change follow the same trend. This 
+    aligns with our insights from the correlation matrix. 
+"""
+
+## --- Conclusion --- ##
+"""
+Conclusion: 
+-------------
+Key findings:
+ - No significant change detected in review volume, rating and sentiment (polarity) before vs during promotional events
+ periods at USS.
+ - This suggests that promotional events by USS did not significantly affect review volume, ratings, or sentiment, 
+ indicating that current marketing strategies may not be effectively influencing guest satisfaction during events.
+
+In summary, for a campaign to improve guest satisfaction, USS should focus on fostering positive guest experiences rather
+than traditional campaign success metrics like engagement rate etc.
+"""
+
+
