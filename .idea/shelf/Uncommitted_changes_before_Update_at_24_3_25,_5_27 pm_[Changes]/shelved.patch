Index: Scripts/Subgroup_A/promo_events_analysis.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>## ----- Set up ----- ##\r\nimport numpy as np\r\nimport pandas as pd\r\nimport os\r\nfrom datetime import datetime, timedelta\r\nfrom emoji import demojize\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nimport nltk\r\nimport corpora\r\n\r\nfrom gensim.models import LdaModel\r\nfrom matplotlib.pyplot import thetagrids\r\nfrom scipy.stats import stats\r\n\r\n# ----- Required downloads -----\r\n# nltk.download('stopwords')\r\n# nltk.download('punkt_tab')\r\n\r\n## ----- Import review and promotion events datasets ----- ##\r\npath = os.getcwd()\r\n# df_reviews = pd.read_csv(path + '/data/universal_studio_branches.csv')\r\ndf_events = pd.read_csv(path + '/data/uss_promo_events.csv')\r\n\r\n## ----- Data cleaning ----- ##\r\n# (1) Check for missing values\r\ndef handle_missing_values(df, drop=True, fill_value=None):\r\n    missing_counts = df.isnull().sum()\r\n    if missing_counts.sum() == 0:\r\n        print(\"No missing values found.\")\r\n        return df\r\n    print(\"Missing values per column:\\n\", missing_counts)\r\n    if drop:\r\n        df = df.dropna()\r\n        print(\"Dropped rows with missing values.\")\r\n    elif fill_value is not None:\r\n        df = df.fillna(fill_value)\r\n        print(f\"Filled missing values with {fill_value}.\")\r\n\r\n    return df\r\n\r\ndf_reviews = handle_missing_values(df_reviews)\r\ndf_events = handle_missing_values(df_events)\r\n\r\n# (2) Convert dates to datetime type\r\ndf_reviews[\"written_date\"] = pd.to_datetime(df_reviews[\"written_date\"], errors='coerce')\r\ndf_events[\"start\"] = pd.to_datetime(df_events[\"start\"], format='%b %d, %Y', errors='coerce')\r\ndf_events[\"end\"] = pd.to_datetime(df_events[\"end\"], format='%b %d, %Y', errors='coerce')\r\n\r\ndf_events = df_events.sort_values(\"start\")  # order events by start date\r\n\r\n# (3) Handle duplicates\r\ndef remove_duplicates(df):\r\n    # check if there are duplicated rows\r\n    dup_count = df.duplicated().sum()\r\n    print(f\"Number of duplicate rows: {dup_count}\")\r\n\r\n    # drop duplicates if any\r\n    if dup_count > 0:\r\n        df.drop_duplicates(inplace=True)\r\n        print(\"Duplicates removed.\")\r\n    else:\r\n        print(\"No duplicates found.\")\r\n    return df\r\n\r\ndf_reviews = remove_duplicates(df_reviews)\r\ndf_events = remove_duplicates(df_events)\r\n\r\n## --- Data exploration --- ##    # edit: added data exploration section\r\n\"\"\"\r\nData exploration\r\n-------------------\r\nSince 'df_events' contains information on past promotion events of Universal Studios Singapore (USS),\r\nwe will filter the 'df_reviews' data to focus our analysis on USS reviews.\r\n\"\"\"\r\n# (1) Compute event duration\r\ndf_events[\"duration\"] = (df_events[\"end\"] - df_events[\"start\"]).dt.days\r\n\r\n# (2) Filter for reviews written during event period\r\ndef filter_reviews_during_events(reviews, events, branch = 'Universal Studios Singapore'):\r\n    event_reviews_dict = {}\r\n\r\n    for _, event in events.iterrows():\r\n        event_key = f\"{event['event']} ({event['start'].date()})\"  # event as unique key\r\n        start = event[\"start\"]\r\n        end = event[\"end\"]\r\n\r\n        filtered_reviews = reviews[\r\n            (reviews[\"written_date\"] >= start) &\r\n            (reviews[\"written_date\"] <= end) &\r\n            (reviews[\"branch\"] == branch)\r\n        ]\r\n\r\n        event_reviews_dict[event_key] = filtered_reviews\r\n\r\n    return event_reviews_dict\r\n\r\n# (3) Filter for reviews written before event period\r\ndef filter_reviews_before_events(reviews, events, branch = 'Universal Studios Singapore'):\r\n    event_reviews_dict = {}\r\n    for _, event in events.iterrows():\r\n        event_key = f\"{event['event']} ({event['start'].date()})\"  # event as unique key\r\n\r\n        # set the period before event to have the same duration for fair comparison\r\n        start = event[\"start\"] - timedelta(days=event[\"duration\"])\r\n        end = event[\"start\"] - timedelta(days=1)\r\n\r\n        filtered_reviews = reviews[\r\n            (reviews[\"written_date\"] >= start) &\r\n            (reviews[\"written_date\"] <= end) &\r\n            (reviews[\"branch\"] == branch)\r\n        ]\r\n\r\n        event_reviews_dict[event_key] = filtered_reviews\r\n\r\n    return event_reviews_dict\r\n\r\n# Call both functions to create 2 filtered dataframes\r\n# Structure of filtered df: {event_key: [df of filtered reviews]}\r\nreviews_during_events = filter_reviews_during_events(df_reviews, df_events)\r\nreviews_before_events = filter_reviews_before_events(df_reviews, df_events)\r\n\r\n# Drop unnecessary column\r\ndf_events = df_events.drop(columns=[\"duration\"])\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Scripts/Subgroup_A/promo_events_analysis.py b/Scripts/Subgroup_A/promo_events_analysis.py
--- a/Scripts/Subgroup_A/promo_events_analysis.py	(revision c4948aebfcfa93ca5d484d37b2c8703d266a2995)
+++ b/Scripts/Subgroup_A/promo_events_analysis.py	(date 1742808437704)
@@ -6,19 +6,23 @@
 from emoji import demojize
 import seaborn as sns
 import matplotlib.pyplot as plt
-import nltk
-import corpora
-
-from gensim.models import LdaModel
-from matplotlib.pyplot import thetagrids
+import kagglehub
 from scipy.stats import stats
 
+
 # ----- Required downloads -----
 # nltk.download('stopwords')
 # nltk.download('punkt_tab')
 
 ## ----- Import review and promotion events datasets ----- ##
-path = os.getcwd()
+path = kagglehub.dataset_download("dwiknrd/reviewuniversalstudio")
+print("Path to dataset:", path)
+
+reviews_path = os.path.join(path, "universal_studio_branches.csv")
+df_reviews = pd.read_csv(reviews_path)
+
+## ----- Import review and promotion events datasets ----- ##
+# path = os.getcwd()
 # df_reviews = pd.read_csv(path + '/data/universal_studio_branches.csv')
 df_events = pd.read_csv(path + '/data/uss_promo_events.csv')
 
